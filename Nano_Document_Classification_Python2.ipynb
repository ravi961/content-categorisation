{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, random, nltk\n",
    "from nltk import bigrams\n",
    "\n",
    "selected_features = None\n",
    "\n",
    "stopwords = ['all', 'just', 'being', 'over', 'both', 'through', 'yourselves', 'its', 'before', 'herself', 'had', \n",
    "             'should', 'to', 'only', 'under', 'ours', 'has', 'do', 'them', 'his', 'very', 'they', 'not', 'during', \n",
    "             'now', 'him', 'nor', 'did', 'this', 'she', 'each', 'further', 'where', 'few', 'because', 'doing', 'some', 'are', \n",
    "             'our', 'ourselves', 'out', 'what', 'for', 'while', 'does', 'above', 'between', 't', 'be', 'we', 'who', \n",
    "             'were', 'here', 'hers', 'by', 'on', 'about', 'of', 'against', 's', 'or', 'own', 'into', 'yourself', \n",
    "             'down', 'your', 'from', 'her', 'their', 'there', 'been', 'whom', 'too', 'themselves', 'was', 'until', \n",
    "             'more', 'himself', 'that', 'but', 'don', 'with', 'than', 'those', 'he', 'me', 'myself', 'these', 'up', \n",
    "             'will', 'below', 'can', 'theirs', 'my', 'and', 'then', 'is', 'am', 'it', 'an', 'as', 'itself', 'at', \n",
    "             'have', 'in', 'any', 'if', 'again', 'no', 'when', 'same','how', 'other', 'which', 'you', 'after', 'most',\n",
    "             'such', 'why', 'a', 'off', 'i', 'yours', 'so', 'the', 'having','once']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_lexical_features(fdist, feature_vector, text):\n",
    "    feature_vector[\"len\"] = len(text)\n",
    "    text_nl = nltk.Text(text)\n",
    "    for word, freq in fdist.items():\n",
    "        fname = \"UNI_\" + word \n",
    "        if selected_features == None or fname in selected_features:        \n",
    "            #feature_vector[fname] = text_nl.count(word)\n",
    "            feature_vector[fname] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features(review_words):\n",
    "    feature_vector = {}\n",
    "\n",
    "    uni_dist = nltk.FreqDist(review_words)\n",
    "    my_bigrams = list(bigrams(review_words))\n",
    "    bi_dist = nltk.FreqDist(my_bigrams)\n",
    "    \n",
    "    add_lexical_features(uni_dist,feature_vector, review_words)\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"dataV4.txt\", 'rb') as f:\n",
    "    text = f.read()\n",
    "text = text.decode(\"utf-8\")\n",
    "#raw_data = text.split(\"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to train the model\n",
    "\n",
    "#block = '0' * 409600\n",
    "#fd = file('dataV1.txt', 'rb')\n",
    "#for x in range(1000):\n",
    " #   fd.read()\n",
    "  #  fd.close()\n",
    "#raw_data = fd.read().decode(\"latin1\")\n",
    "#with open(\"dataV1.txt\", \"rt\") as f:      \n",
    " #   raw_data = f.read().splitlines()\n",
    "  #  f.close()\n",
    "    \n",
    "    #content = f.read().splitlines()\n",
    "\n",
    "#with open( \"dataV1.txt\" ) as infile:\n",
    "  # read first line and remember it\n",
    "  #  first_line = infile.read()\n",
    "  #  raw_data = first_line.decode('iso8859-15', 'ignore')\n",
    "\n",
    "\n",
    "#file = open(\"dataV1.txt\", \"rb\", encoding = \"latin1\")\n",
    "#data = file.read()\n",
    "#file.close()\n",
    "#line = f.readline()\n",
    "#  while line:\n",
    "    # do stuff with line\n",
    "   #   raw_data = f.read().decode(\"latin1\")\n",
    "#    line = file.readline()\n",
    "#file.close()\n",
    "#raw_data = file.read().decode('iso8859-15', 'ignore')\n",
    "#text = data.decode(\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = text.split(\"\\n\")\n",
    "docs2 = docs[1: ]\n",
    "train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in docs2:\n",
    "    d = d.split()\n",
    "    if len(d)!=0:\n",
    "        cl = d[0]\n",
    "        text_d = d[1: ]#we need to remove the stopwords\n",
    "        text = []\n",
    "        for w in text_d:\n",
    "            if w not in stopwords:\n",
    "                text.append(w)\n",
    "        item = (text, cl)\n",
    "        train.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = train[ :1576]\n",
    "valid_set = train[1577: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets_tr = [(features(words), label) for (words, label) in train_set ]\n",
    "featuresets_val = [(features(words), label) for (words, label) in valid_set ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(features(words), label) for (words, label) in train ]\n",
    "#print(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(featuresets_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(featuresets)\n",
    "#classifier.show_most_informative_features(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#accuracy = nltk.classify.accuracy(classifier, featuresets_tr)\n",
    "#print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to take input\n",
    "n = int(input())\n",
    "a = []\n",
    "for a_i in range(n): # to read a matrix\n",
    "    a_t = [a_temp for a_temp in input().strip().split(' ')]\n",
    "    a.append(a_t)\n",
    "    \n",
    "featuresets_test = [features(words) for words in a ]  \n",
    "\n",
    "predicted_labels = classifier.classify_many(featuresets_test)\n",
    "for l in predicted_labels:\n",
    "    print (str(\"Type of input data: \"+l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
